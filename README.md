<h1 align="center">ANLISIS Y CUADROS DE MANDO</h1>

![Banner para el README.md](https://repository-images.githubusercontent.com/588181932/e36ec678-7984-4cdd-8e4c-a3932772ff8e)

> **Profesor:** Alberto M谩rquez Alarc贸n - [@amarala931](https://github.com/amarala931).

##  Miembros del Equipo

- Andr茅s Prado Morgaz - [@andpramor](https://github.com/andpramor).
- Manuel Jes煤s de la Rosa Cosano - [@Nastupiste](https://github.com/Nastupiste).

---

## 3.1. Representaci贸n y Estructura de Datos

###  Objetivos

En la actividad 3.1, estos son los objetivos:

1. **Extraer** informaci贸n de la fuente de datos creada anteriormente integr谩ndola en un flujo de Python.

2. **Dominar la manipulaci贸n de DataFrames con Polars**, aplicando filtros, agregaciones y transformaciones complejas.

3. **Dise帽ar visualizaciones interactivas avanzadas** que permitan identificar tendencias, patrones y valores at铆picos (outliers) en los datos sobre el poder adquisitivo y el empleo.

4. **Mantener el ciclo de vida del software** mediante el uso de forks en Git y GitHub para la colaboraci贸n y el control de versiones.

---

###  Pasos

- [x] **Paso 0**. Prerrequisitos: Base de datos.
  - [x] Adaptar el ejercicio a sqlite3 (la MongoDB de la actividad 1.7. ya se ha borrado de la capa gratuita de MongoAtlas).
  - [x] Poblar la nueva Base de Datos.
  - [x] Documentar en este README c贸mo instalar las dependencias de este proyecto.

- [x] **Paso 1**. Conexi贸n.
  - [x] Establecer la conexi贸n entre el entorno de Python y la base de datos.
  - [x] Extraer datos y cargarlos en un objeto de Polars (utilizando el m茅todo de Polars: read_database).

- [x] **Paso 2**. Limpieza y Estructuraci贸n con Polars.
  - [x] Tratamiento de valores nulos o inconsistentes.
  - [x] Creaci贸n de columnas calculadas: hemos creado columnas para temperatura m谩xima, m铆nima y mediana.
  - [x] Agrupaciones (GroupBy) para segmentar la informaci贸n: lo hemos hecho por ID en df_stats.
- DUDA: Agrupaciones 驴A帽adimos por a帽os?
- DUDA: 驴Los datos se duplican al correr varias veces la extracci贸n en menos de 12 horas (la API nos da el hourly de 12 horas)? En main.py hay una l铆nea comentada para hacer una extracci贸n nueva.

- [x] **Paso 3**. Generaci贸n de Dataframes para Informes.
  - [x] Exportar archivos CSV con el contenido de cada dataframe.

- [ ] **Paso 4**. An谩lisis visual con Plotly.
  - [ ] Gr谩ficos de l铆neas interactivos para ver la evoluci贸n temporal.
  - [ ] Scatter plots (diagramas de dispersi贸n) para ver la correlaci贸n entre dos variables.
  - [ ] Gr谩ficos facetados (subplots) para comparar distintas regiones o indicadores simult谩neamente.

- [ ] **Paso 5**. Documentaci贸n y Sincronizaci贸n.
  - [x] Actualizar el repositorio de GitHub, incluyendo el requirements.txt.
  - [ ] Volver a actualizar `requirements.txt` al incluir Plotly.
  - [ ] Documentar en este README.md las visualizaciones generadas y conclusiones preliminares obtenidas.

- [ ] **Final**. Revisar documentaci贸n.
  - [ ] README completo.
  - [x] `requirements.txt` actualizado.
  - [ ] An谩lisis de los resultados incluido en README.

---

##  Instalaci贸n de dependencias

### Utilizando `uv`

Tras clonar el repositorio en local, abrimos una terminal en la ra铆z del proyecto y ejecutamos:

```bash
uv sync
```

Esto genera un entorno virtual en la ra铆z del proyecto e instala las dependencias listadas en `pyproject.toml`.

### Utilizando `pip`

Generamos un entorno virtual (`python -m venv <nombre_del_entorno>`), lo activamos con `.\<nombre_del_entorno>\Scripts\activate` (Windows) o `source <nombre_del_entorno>/bin/activate` (MacOS o Linux).

Hecho esto, ejecutamos:

```bash
pip install -r requirements.txt
```

> NOTA: Hemos utilizado `uv`, por lo que hemos generado el archivo `requirements.txt` de este proyecto ejecutando:

```bash
uv export --format requirements-txt --no-hashes --no-annotate --no-header --output-file requirements.txt
```

---

##  Ejecuci贸n del proyecto

Con las dependencias instaladas y el entorno virtual activado, ejecutamos el archivo `main.py`:

```bash
python .\main.py
```
